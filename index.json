
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Hello! I am a fifth-year PhD student in computer science at the University of Washington, advised by Yejin Choi and Noah Smith. My research area is natural language processing, with interests particularly in tokenization, decoding-time algorithms, and data creation. I am grateful to be supported by the NSF Graduate Research Fellowship and OpenAI SuperAlignment Fellowship.\nPreviously I graduated from Northwestern University with majors in computer science and math, where I was very fortunate to be mentored by Professor Doug Downey, Professor Bryan Pardo, and Dr. Prem Seetharaman.\n","date":1752192000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1752192000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hello! I am a fifth-year PhD student in computer science at the University of Washington, advised by Yejin Choi and Noah Smith. My research area is natural language processing, with interests particularly in tokenization, decoding-time algorithms, and data creation.","tags":null,"title":"Alisa Liu","type":"authors"},{"authors":null,"categories":null,"content":"","date":1752019200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1752019200,"objectID":"c529841b8d115f5b6a5748bc4785fb75","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"","tags":null,"title":"Alisa Liu*","type":"authors"},{"authors":null,"categories":null,"content":"","date":1752019200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1752019200,"objectID":"0753073516c1480e76b8e0520f3e10fc","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"","tags":null,"title":"Jonathan Hayase*","type":"authors"},{"authors":["Jonathan Hayase","Alisa Liu","Noah A. Smith","Sewoong Oh"],"categories":null,"content":"","date":1752192000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752192000,"objectID":"425b4036267f78f5c26c8d86b4a7babc","permalink":"https://alisawuffles.github.com/publication/byte_sampler/","publishdate":"2025-07-11T00:00:00Z","relpermalink":"/publication/byte_sampler/","section":"publication","summary":"Tokenization is used almost universally by modern language models, enabling efficient text representation using multi-byte or multi-character tokens. However, prior work has shown that tokenization can introduce distortion into the model's generations. For example, users are often advised not to end their prompts with a space because it prevents the model from including the space as part of the next token. This Prompt Boundary Problem (PBP) also arises in languages such as Chinese and in code generation, where tokens often do not line up with syntactic boundaries. Additionally mismatching tokenizers often hinder model composition and interoperability. For example, it is not possible to directly ensemble models with different tokenizers due to their mismatching vocabularies. To address these issues, we present an inference-time method to convert any autoregressive LM with a BPE tokenizer into a character-level or byte-level LM, without changing its generative distribution at the text level. Our method efficient solves the PBP and is also able to unify the vocabularies of language models with different tokenizers, allowing one to ensemble LMs with different tokenizers at inference time as well as transfer the post-training from one model to another using proxy-tuning. We demonstrate in experiments that the ensemble and proxy-tuned models outperform their constituents on downstream evals.","tags":["tokenization"],"title":"Sampling from Your Language Model One Byte at a Time","type":"publication"},{"authors":["Brian Siyuan Zheng","Alisa Liu","Orevaoghene Ahia","Jonathan Hayase","Yejin Choi","Noah A. Smith"],"categories":null,"content":"","date":1752105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752105600,"objectID":"11a86e93c50a7790aeb156e9e5f3925d","permalink":"https://alisawuffles.github.com/publication/broken_tokens/","publishdate":"2025-07-10T00:00:00Z","relpermalink":"/publication/broken_tokens/","section":"publication","summary":"Modern tokenizers employ deterministic algorithms to map text into a single \"canonical\" token sequence, yet the same string can be encoded as many non-canonical tokenizations using the tokenizer vocabulary. In this work, we investigate the robustness of LMs to text encoded with non-canonical tokenizations entirely unseen during training. Surprisingly, when evaluated across 20 benchmarks, we find that instruction-tuned models retain up to 93.4% of their original performance when given a randomly sampled tokenization, and 90.8% with character-level tokenization. We see that overall stronger models tend to be more robust, and robustness diminishes as the tokenization departs farther from the canonical form. Motivated by these results, we then identify settings where non-canonical tokenization schemes can *improve* performance, finding that character-level segmentation improves string manipulation and code understanding tasks by up to +14%, and right-aligned digit grouping enhances large-number arithmetic by +33%. Finally, we investigate the source of this robustness, finding that it arises in the instruction-tuning phase. We show that while both base and post-trained models grasp the semantics of non-canonical tokenizations (perceiving them as containing misspellings), base models try to mimic the imagined mistakes and degenerate into nonsensical output, while post-trained models are committed to fluent responses. Overall, our findings suggest that models are less tied to their tokenizer than previously believed, and demonstrate the promise of intervening on tokenization at inference time to boost performance.","tags":["tokenization"],"title":"Broken Tokens? Your Language Model can Secretly Handle Non-Canonical Tokenizations","type":"publication"},{"authors":["Alisa Liu*","Jonathan Hayase*","Sewoong Oh","Noah A. Smith","Yejin Choi"],"categories":null,"content":"","date":1752019200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752019200,"objectID":"55235e9e125f6cff1cbba7cf91158e05","permalink":"https://alisawuffles.github.com/publication/superbpe/","publishdate":"2025-07-09T00:00:00Z","relpermalink":"/publication/superbpe/","section":"publication","summary":"The assumption across nearly all language model (LM) tokenization schemes is that tokens should be subwords, i.e., contained within word boundaries. While providing a seemingly reasonable inductive bias, is this common practice limiting the potential of modern LMs? Whitespace is not a reliable delimiter of meaning, as evidenced by multi-word expressions (e.g., \"by the way\"), crosslingual variation in the number of words needed to express a concept (e.g., \"spacesuit helmet\" in German is \"raumanzughelm\"), and languages that do not use whitespace at all (e.g., Chinese). To explore the potential of tokenization beyond subwords, we introduce a \"superword\" tokenizer, SuperBPE, which incorporates a simple pretokenization curriculum into the byte-pair encoding (BPE) algorithm to first learn subwords, then superwords that bridge whitespace. This brings dramatic improvements in encoding efficiency\\: when fixing the vocabulary size to 200k, SuperBPE encodes a fixed piece of text with up to 33% fewer tokens than BPE on average. In experiments, we pretrain 8B transformer LMs from scratch while fixing the model size, vocabulary size, and train compute, varying *only* the algorithm for learning the vocabulary. Our model trained with SuperBPE achieves an average +4.0% absolute improvement over the BPE baseline across 30 downstream tasks (including +8.2% on MMLU), while simultaneously requiring 27% less compute at inference time. In analysis, we find that SuperBPE results in segmentations of text that are more uniform in per-token difficulty. Qualitatively, this may be because SuperBPE tokens often capture common multi-word expressions that function semantically as a single unit. SuperBPE is a straightforward, local modification to tokenization that improves both encoding efficiency and downstream performance, yielding better language models overall.","tags":["tokenization"],"title":"SuperBPE: Space Travel for Language Models","type":"publication"},{"authors":["Nathan Lambert","Jacob Morrison","Valentina Pyatkin","Shengyi Huang","Hamish Ivison","Faeze Brahman","Lester James V. Miranda","Alisa Liu","Nouha Dziri","Shane Lyu","Yuling Gu","Saumya Malik","Victoria Graf","Jena D. Hwang","Jiangjiang Yang","Ronan Le Bras","Oyvind Tafjord","Chris Wilhelm","Luca Soldaini","Noah A. Smith","Yizhong Wang","Pradeep Dasigi","Hannaneh Hajishirzi"],"categories":null,"content":"","date":1751932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751932800,"objectID":"622a9f48b0b6ae6e614765448d94447b","permalink":"https://alisawuffles.github.com/publication/tulu3/","publishdate":"2025-07-08T00:00:00Z","relpermalink":"/publication/tulu3/","section":"publication","summary":"Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce Tulu 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. Tulu 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With Tulu 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance. In addition to the Tulu 3 model weights and demo, we release the complete recipe -- including datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting the Tulu 3 approach to more domains.","tags":["post-training"],"title":"Tulu 3: Pushing Frontiers in Open Language Model Post-Training","type":"publication"},{"authors":["Hila Gonen","Terra Blevins","Alisa Liu","Luke Zettlemoyer","Noah A. Smith"],"categories":null,"content":"","date":1746835200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1746835200,"objectID":"4190b1c6c91220b4aa86e13be048480f","permalink":"https://alisawuffles.github.com/publication/semantic_leakage/","publishdate":"2025-05-10T00:00:00Z","relpermalink":"/publication/semantic_leakage/","section":"publication","summary":"Despite their wide adoption, the biases and unintended behaviors of language models remain poorly understood. In this paper, we identify and characterize a phenomenon never discussed before, which we call semantic leakage, where models leak irrelevant information from the prompt into the generation in unexpected ways. We propose an evaluation setting to detect semantic leakage both by humans and automatically, curate a diverse test suite for diagnosing this behavior, and measure significant semantic leakage in 13 flagship models. We also show that models exhibit semantic leakage in languages besides English and across different settings and generation scenarios. This discovery highlights yet another type of bias in language models that affects their generation patterns and behavior.","tags":["post-training"],"title":"Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models","type":"publication"},{"authors":["Tuochao Chen","Nicholas Batchelder","Alisa Liu","Noah Smith","Shyamnath Gollakota"],"categories":null,"content":"","date":1746576000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1746576000,"objectID":"883eb757b5c33c7c093fa80af30bda6b","permalink":"https://alisawuffles.github.com/publication/llamapie/","publishdate":"2025-05-07T00:00:00Z","relpermalink":"/publication/llamapie/","section":"publication","summary":"We introduce LlamaPIE, the first real-time proactive assistant designed to enhance human conversations through discreet, concise guidance delivered via hearable devices. Unlike traditional language models that require explicit user invocation, this assistant operates in the background, anticipating user needs without interrupting conversations. We address several challenges, including determining when to respond, crafting concise responses that enhance conversations, leveraging knowledge of the user for context-aware assistance, and real-time, on-device processing. To achieve this, we construct a semi-synthetic dialogue dataset and propose a two-model pipeline: a small model that decides when to respond and a larger model that generates the response. We evaluate our approach on real-world datasets, demonstrating its effectiveness in providing helpful, unobtrusive assistance. User studies with our assistant, implemented on Apple Silicon M2 hardware, show a strong preference for the proactive assistant over both a baseline with no assistance and a reactive model, highlighting the potential of LlamaPie to enhance live conversations.","tags":null,"title":"LlamaPIE: Proactive In-Ear Conversation Assistants","type":"publication"},{"authors":["Jonathan Hayase*","Alisa Liu*","Yejin Choi","Sewoong Oh","Noah A. Smith"],"categories":null,"content":"","date":1721692800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721692800,"objectID":"cf3c1dcbde1a204717740f4928d2d2c5","permalink":"https://alisawuffles.github.com/publication/bpe/","publishdate":"2024-07-23T00:00:00Z","relpermalink":"/publication/bpe/","section":"publication","summary":"Despite the general capabilities of large pretrained language models, they consistently benefit from further adaptation to better achieve desired behaviors. However, tuning these models has become increasingly resource-intensive, or impossible when model weights are private. We introduce proxy-tuning, a lightweight decoding-time algorithm that operates on top of black-box LMs to achieve the result of directly tuning the model, but by accessing only its prediction over the output vocabulary. Our method instead tunes a smaller LM, then applies the difference between the predictions of the small tuned and untuned LMs to shift the original predictions of the base model in the direction of tuning, while retaining the benefits of larger scale pretraining. In experiments, when we apply proxy-tuning to Llama2-70B using proxies of only 7B size, we can close 88% of the gap between Llama2-70B and its truly-tuned chat version, when evaluated across knowledge, reasoning, and safety benchmarks. Interestingly, when tested on TruthfulQA, proxy-tuned models are actually more truthful than directly tuned models, possibly because decoding-time guidance better retains the model's factual knowledge. We then demonstrate the generality of proxy-tuning by applying it for domain adaptation on code, and task-specific finetuning on question-answering and math problems. Our work demonstrates the promise of using small tuned LMs to efficiently customize large, potentially proprietary LMs through decoding-time guidance.","tags":null,"title":"Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?","type":"publication"},{"authors":["Alisa Liu","Xiaochuang Han","Yizhong Wang","Yulia Tsvetkov","Yejin Choi","Noah A. Smith"],"categories":null,"content":"","date":1705363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705363200,"objectID":"2b122eeb97a173ca4cc61d4ab42b76e9","permalink":"https://alisawuffles.github.com/publication/proxy_tuning/","publishdate":"2024-01-16T00:00:00Z","relpermalink":"/publication/proxy_tuning/","section":"publication","summary":"We develop an algorithm for \"tuning\" language models at decoding-time!","tags":["text generation"],"title":"Tuning Language Models by Proxy","type":"publication"},{"authors":["Alisa Liu","Zhaofeng Wu","Julian Michael","Alane Suhr","Peter West","Alexander Koller","Swabha Swayamdipta","Noah A. Smith","Yejin Choi"],"categories":null,"content":"","date":1697760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697760000,"objectID":"b48b43b64ec2bc2af513d17233ee3383","permalink":"https://alisawuffles.github.com/publication/ambient/","publishdate":"2023-10-20T00:00:00Z","relpermalink":"/publication/ambient/","section":"publication","summary":"We build a benchmark to evaluate LM understanding of ambiguity, which is an intrinsic feature of language, and find that the task remains extremely challenging, including for GPT-4","tags":["ambiguity","dataset","evaluation","natural language inference"],"title":"We're Afraid Language Models Aren't Modeling Ambiguity","type":"publication"},{"authors":["Jaechan Lee","Alisa Liu","Orevaoghene Ahia","Hila Gonen","Noah A. Smith"],"categories":null,"content":"","date":1697673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697673600,"objectID":"c5058736da31a5b27cf00c2aa60c496c","permalink":"https://alisawuffles.github.com/publication/tide/","publishdate":"2023-10-19T00:00:00Z","relpermalink":"/publication/tide/","section":"publication","summary":"The translation of ambiguous text presents a challenge for translation systems, as it requires using the surrounding context to disambiguate the intended meaning as much as possible. While prior work has studied ambiguities that result from different *grammatical* features of the source and target language, we study semantic ambiguities that exist in the source (English in this work) itself. In particular, we focus on idioms that are open to both literal and figurative interpretations (e.g., *goose egg*), and collect TIDE, a dataset of 512 pairs of English sentences containing idioms with disambiguating context such that one is literal (*it laid a goose egg*) and another is figurative (*they scored a goose egg*, as in a score of zero). In experiments, we evaluate neural MT models and language models for (i) their **preference** when given an ambiguous subsentence, (ii) their **sensitivity** to disambiguating context, and (iii) the performance **disparity** between figurative and literal source sentences. We find that current MT models consistently translate English idioms literally, even when the context suggests a figurative interpretation. On the other hand, LMs are far more context-aware, although there remain disparities across target languages. Our findings underline the potential of LMs as a strong backbone for context-aware translation.","tags":["ambiguity","dataset","evaluation"],"title":"That was the last straw, we need more: Are Translation Systems Sensitive to Disambiguating Context?","type":"publication"},{"authors":["Ian R. McKenzie","18 others","Alisa Liu","Jiacheng Liu","Tom Tseng","Tomasz Korbak","Najoung Kim","Samuel R. Bowman","Ethan Perez"],"categories":null,"content":"","date":1697587200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697587200,"objectID":"d389bdd8c8c7b9a29ca9db0a7d2ee145","permalink":"https://alisawuffles.github.com/publication/inverse_scaling/","publishdate":"2023-10-18T00:00:00Z","relpermalink":"/publication/inverse_scaling/","section":"publication","summary":"Work on scaling laws has found that large language models (LMs) show predictable improvements to overall loss with increased scale (model size, training data, and compute). Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data. We present empirical evidence of inverse scaling on 11 datasets collected by running a public contest, the Inverse Scaling Prize, with a substantial prize pool. Through analysis of the datasets, along with other examples found in the literature, we identify four potential causes of inverse scaling: (i) preference to repeat memorized sequences over following in-context instructions, (ii) imitation of undesirable patterns in the training data, (iii) tasks containing an easy distractor task which LMs could focus on, rather than the harder real task, and (iv) correct but misleading few-shot demonstrations of the task. We release the winning datasets at inversescaling.com/data to allow for further investigation of inverse scaling. Our tasks have helped drive the discovery of U-shaped and inverted-U scaling trends, where an initial trend reverses, suggesting that scaling trends are less reliable at predicting the behavior of larger-scale models than previously understood. Overall, our results suggest that there are tasks for which increased model scale alone may not lead to progress, and that more careful thought needs to go into the data and objectives for training language models.","tags":["dataset","evaluation"],"title":"Inverse Scaling: When Bigger Isn't Better","type":"publication"},{"authors":["Muru Zhang","Ofir Press","William Merrill","Alisa Liu","Noah A. Smith"],"categories":null,"content":"","date":1684713600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684713600,"objectID":"0105475463132d065c8a017bddcd4b8c","permalink":"https://alisawuffles.github.com/publication/snowballing/","publishdate":"2023-05-22T00:00:00Z","relpermalink":"/publication/snowballing/","section":"publication","summary":"A major risk of using language models in practical applications is their tendency to hallucinate incorrect statements. Hallucinations are often attributed to knowledge gaps in LMs, but we show that LMs sometimes produce hallucinations that they can separately recognize as incorrect. To do this, we construct three question-answering datasets where LMs often state an incorrect answer which is followed by an explanation with at least one incorrect claim. Crucially, we find that GPT-3.5, GPT-4, and LLaMA2-70B-chat can identify 67%, 87%, and 94% of these incorrect claims, respectively. We show that this phenomenon doesn't disappear under higher temperatures sampling, beam search, and zero-shot chain-of-thought prompting. These findings reveal that LM hallucinations can snowball: early mistakes by an LM can lead to more mistakes that otherwise would not be made.","tags":["hallucination"],"title":"How Language Model Hallucinations Can Snowball","type":"publication"},{"authors":["Yizhong Wang","Yeganeh Kordi","Swaroop Mishra","Alisa Liu","Noah A. Smith","Daniel Khashabi","Hannaneh Hajishirzi"],"categories":null,"content":"","date":1671580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671580800,"objectID":"65fd5b50fdcad78d1167e05b9aebb209","permalink":"https://alisawuffles.github.com/publication/self_instruct/","publishdate":"2022-12-21T00:00:00Z","relpermalink":"/publication/self_instruct/","section":"publication","summary":"Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.","tags":["text generation","dataset","instruction following"],"title":"Self-Instruct: Aligning Language Models with Self-Generated Instructions","type":"publication"},{"authors":["Skyler Hallinan","Alisa Liu","Yejin Choi","Maarten Sap"],"categories":null,"content":"","date":1671494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671494400,"objectID":"72bd9d47190ac45611b625fb662c3ba5","permalink":"https://alisawuffles.github.com/publication/marco/","publishdate":"2022-12-20T00:00:00Z","relpermalink":"/publication/marco/","section":"publication","summary":"Using expert and anti-expert LMs to rewrite toxic text for safety","tags":["text generation","toxicity"],"title":"Detoxifying Text with MaRCo: Controllable Revision with Experts and Anti-Experts","type":"publication"},{"authors":["Alisa Liu","Swabha Swayamdipta","Noah A. Smith","Yejin Choi"],"categories":null,"content":"","date":1642204800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642204800,"objectID":"756e63fd5afe5ba0196006ae27fe679a","permalink":"https://alisawuffles.github.com/publication/wanli/","publishdate":"2022-01-15T00:00:00Z","relpermalink":"/publication/wanli/","section":"publication","summary":"We introduce a paradigm for dataset creation based on human and machine collaboration, and demonstrate its empirical effectiveness for collecting a new large-scale NLI dataset","tags":["dataset","text generation","natural language inference"],"title":"WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation","type":"publication"},{"authors":["Jiacheng Liu","Alisa Liu","Ximing Lu","Sean Welleck","Peter West","Ronan Le Bras","Yejin Choi","Hannaneh Hajishirzi"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"0b33cac39a523ad281eceee3b454f54e","permalink":"https://alisawuffles.github.com/publication/knowledge_gen/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/publication/knowledge_gen/","section":"publication","summary":"Prompting GPT-3 to generate relevant background knowledge improves performance on a variety of commonsense reasoning tasks","tags":["text generation","commonsense reasoning"],"title":"Generated Knowledge Prompting for Commonsense Reasoning","type":"publication"},{"authors":["Alisa Liu","Maarten Sap","Ximing Lu","Swabha Swayamdipta","Chandra Bhagavatula","Noah A. Smith","Yejin Choi"],"categories":null,"content":"","date":1627862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627862400,"objectID":"0603c839ecc8847a0334bc6cedb0e026","permalink":"https://alisawuffles.github.com/publication/dexperts/","publishdate":"2021-08-02T00:00:00Z","relpermalink":"/publication/dexperts/","section":"publication","summary":"Steering open-ended text generation toward desired or away from undesired attributes, using expert and anti-expert language models","tags":["text generation","toxicity"],"title":"DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts","type":"publication"},{"authors":["Alisa Liu","Prem Seetharaman","Bryan Pardo"],"categories":null,"content":"","date":1604275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604275200,"objectID":"5670b4eefae894fde84d28a2990337f4","permalink":"https://alisawuffles.github.com/publication/ensemble/","publishdate":"2020-11-02T00:00:00Z","relpermalink":"/publication/ensemble/","section":"publication","summary":"Ensemble model for audio source separation, using a confidence measure to mediate among domain-specific models","tags":["source separation","deep clustering"],"title":"Model Selection for Deep Audio Source Separation via Clustering Analysis","type":"publication"},{"authors":["Alex Fang","Alisa Liu","Prem Seetharaman","Bryan Pardo"],"categories":null,"content":"","date":1595030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595030400,"objectID":"2a6c41a4f969a5982c323f6a62c270d4","permalink":"https://alisawuffles.github.com/publication/grading_function/","publishdate":"2020-07-18T00:00:00Z","relpermalink":"/publication/grading_function/","section":"publication","summary":"An automatic, interpretable, and musically-motivated grading function for Bach chorales","tags":["music generation"],"title":"Bach or Mock? A Grading Function for Chorales in the Style of J.S. Bach","type":"publication"},{"authors":["Alisa Liu","Alex Fang","Gaëtan Hadjeres","Prem Seetharaman","Bryan Pardo"],"categories":null,"content":"","date":1595030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595030400,"objectID":"84858b3d3275032965c11e31f112e34a","permalink":"https://alisawuffles.github.com/publication/aug_gen/","publishdate":"2020-07-18T00:00:00Z","relpermalink":"/publication/aug_gen/","section":"publication","summary":"A generative data augmentation method for music generation systems on a resource-constrained domain","tags":["music generation"],"title":"Incorporating Music Knowledge in Continual Dataset Augmentation for Music Generation","type":"publication"},{"authors":["Alisa Liu"],"categories":null,"content":"I really enjoy making study guides! Here are a couple I’m especially proud of, and I hope they are useful resources for other students.\nEECS 396: Statistical Machine Learning The professor shared the topic being tested by every problem on the final, so I organized course content around it.\nMath 312: Number Theory One of my favorite classes (and subjects) of all time. I shared this with the class and passed it on to many students who would take the course after me. I still hear from people who use it, which makes me really happy.\nMath 320: Real Analysis This class ⁠makes robust everything we know and love from calculus. In this study guide I try to outline approaches to common problems and provide examples of functions with interesting properties.\nMath 307: Applications of Linear Algebra More of a formula sheet than a study guide.\n","date":1573257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573257600,"objectID":"10648a30886333134259f643769e3276","permalink":"https://alisawuffles.github.com/post/study-guides/","publishdate":"2019-11-09T00:00:00Z","relpermalink":"/post/study-guides/","section":"post","summary":"I really enjoy making study guides! Here are a couple I’m especially proud of, and I hope they are useful resources for other students.\nEECS 396: Statistical Machine Learning The professor shared the topic being tested by every problem on the final, so I organized course content around it.","tags":null,"title":"Study guides from undergrad","type":"post"},{"authors":["Michael Chen","Mike D’Arcy","Alisa Liu","Jared Fernandez","Doug Downey"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"be980448a0cf79ef2ae03d613777bb02","permalink":"https://alisawuffles.github.com/publication/codah/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/publication/codah/","section":"publication","summary":"An adversarially-constructed dataset for common sense QA","tags":["commonsense reasoning","dataset"],"title":"CODAH: An Adversarially-Authored Question Answering Dataset for Common Sense","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://alisawuffles.github.com/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://alisawuffles.github.com/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"}]